{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6999196-76f4-4c6e-a687-25b6cf399dde",
   "metadata": {},
   "source": [
    "## 학습용 데이터 실습: Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a7f1a1-9e39-4ef6-a586-3a4d287ab98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "l_data=datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e422862a-05c5-4cff-ad9b-427490c8a410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n"
     ]
    }
   ],
   "source": [
    "print(l_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc0535b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "    Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "    Structure and Classification Rule for Recognition in Partially Exposed\n",
      "    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "    on Information Theory, May 1972, 431-433.\n",
      "  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "    conceptual clustering system finds 3 classes in the data.\n",
      "  - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(l_data['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf00ed33",
   "metadata": {},
   "source": [
    "### 붓꽃 데이터\n",
    "\n",
    "- 인스턴스 수: 150(세가지 클래스에 대해 각각 50 개 씩)\n",
    "- 속성: 4 숫자형\n",
    "- 속성 정보\n",
    "    - 꽃받침(sepal) 길이[cm]\n",
    "    - 꽃받침(sepal) 너비[cm]\n",
    "    - 꽃잎(petal) 길이[cm]\n",
    "    - 꽃잎(petal) 너비[cm]\n",
    "- 클래스\n",
    "    - 붓꽃: 세토사\n",
    "    - 붓꽃: 버시컬러\n",
    "    - 붓꽃: 버지니카\n",
    "\n",
    "#### 설명\n",
    "패턴 인식관련 페이퍼에 중에서는 가장 유명한 데이터셋. R.A Fisher에 의해 처음 등장했으며, Fisher의 이 논문은 현재까지도 고전으로, 많은 논문에서 인용하고 있다. UCI Machin Learning Repository에서와는 달리 두가지의 오류 포인트가 수정된 버전이다. 하나의 클래스는 다른 두 클래스와 linearly separable하다.\n",
    "\n",
    "[참고: linear separablility](https://en.wikipedia.org/wiki/Linear_separability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b072ab66-c119-454d-b2c2-89f04f18ca4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(l_data['feature_names'])\n",
    "print(l_data['target_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea47a854-84eb-4234-809f-21b15509cd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. dropdown:: References\\n\\n  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n    Mathematical Statistics\" (John Wiley, NY, 1950).\\n  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n    Structure and Classification Rule for Recognition in Partially Exposed\\n    Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n    on Information Theory, May 1972, 431-433.\\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n    conceptual clustering system finds 3 classes in the data.\\n  - Many, many more ...\\n',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "207bd1e5-ecc8-4c41-8a59-14662c421900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     target  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "..      ...  \n",
       "145       2  \n",
       "146       2  \n",
       "147       2  \n",
       "148       2  \n",
       "149       2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(l_data['data'], columns=l_data['feature_names'])\n",
    "df['target']=l_data['target']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e513d41a-e4cc-4913-b84b-df35880a2f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    50\n",
      "1    50\n",
      "2    50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['target'].value_counts()) # 각 타겟별 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5470683-7b57-4588-9a0b-eb2bd625cc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== train target ==============================\n",
      "target\n",
      "0         50\n",
      "1         50\n",
      "2         20\n",
      "Name: count, dtype: int64\n",
      "============================== test target ==============================\n",
      "target\n",
      "2         30\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "# 세트는 총 150개, 120개는 train, 나머지 30개는 test로 설정하려고 한다.\n",
    "\n",
    "iris = load_iris()\n",
    "# 데이터프레임으로 정의\n",
    "df = pd.DataFrame(iris['data'], columns=iris['feature_names'])\n",
    "df['target']=iris['target']\n",
    "\n",
    "x_train = df.iloc[:120, :-1]\n",
    "x_test  = df.iloc[120:, :-1]\n",
    "y_train = df.iloc[:120, -1:]\n",
    "y_test  = df.iloc[120:, -1:]\n",
    "print(\"=\"*30, \"train target\", \"=\"*30)\n",
    "# print(\"x_train: \", x_train)\n",
    "# print(\"y_train: \", y_train)\n",
    "print(y_train.value_counts())\n",
    "print(\"=\"*30, \"test target\", \"=\"*30)\n",
    "# print(\"x_test: \", x_test)\n",
    "# print(\"y_test: \", y_test)\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e18000f-337f-4bfd-8120-764f2383401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== train target ==============================\n",
      "target\n",
      "0         42\n",
      "1         39\n",
      "2         39\n",
      "Name: count, dtype: int64\n",
      "============================== test target ==============================\n",
      "target\n",
      "1         11\n",
      "2         11\n",
      "0          8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 랜덤하게 섞는다: 학습셋, 테스트셋에 타겟별 몇 개씩 들어가는가?\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris['data'], columns=iris['feature_names'])\n",
    "df['target']=iris['target']\n",
    "df_shuffled = df.iloc[np.random.permutation(df.index)].reset_index(drop=True)\n",
    "\n",
    "x_train = df_shuffled.iloc[:120, :-1]\n",
    "x_test  = df_shuffled.iloc[120:, :-1]\n",
    "y_train = df_shuffled.iloc[:120, -1:]\n",
    "y_test  = df_shuffled.iloc[120:, -1:]\n",
    "print(\"=\"*30, \"train target\", \"=\"*30)\n",
    "print(y_train.value_counts())\n",
    "print(\"=\"*30, \"test target\", \"=\"*30)\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657c129-88e2-46d8-b821-fd8b62bf22a9",
   "metadata": {},
   "source": [
    "### 실습: 7.1.2. ~ 7.1.6.\n",
    "토이데이터 7.1.2~7.1.6에 대하여 다음을 프로그래밍 소스와 결과를 보이고 다음을 작성하시오\n",
    "\n",
    "1) 무슨 데이터인지 한글로 설명(DESCR)\n",
    "\n",
    "2) feature와 target에 대하여 한글로 설명\n",
    "\n",
    "3) 회귀,판단, 분류중 어떤 것인지 설명\n",
    "\n",
    "4) 전체 데이터수를 파악하고, 판단과 분류라면 타겟별 개수를 출력 설명\n",
    "\n",
    "5) 피쳐와 타겟을 합쳐서 일괄로 보이는 dataframe구성\n",
    "\n",
    "6) train/test set을 70%.30%로 적절하게 분리하는 소스를 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f6f03d",
   "metadata": {},
   "source": [
    "## 7.1.2. Diabetes\n",
    "### 1. DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "499ca4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 442\n",
      "\n",
      ":Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      ":Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      ":Attribute Information:\n",
      "    - age     age in years\n",
      "    - sex\n",
      "    - bmi     body mass index\n",
      "    - bp      average blood pressure\n",
      "    - s1      tc, total serum cholesterol\n",
      "    - s2      ldl, low-density lipoproteins\n",
      "    - s3      hdl, high-density lipoproteins\n",
      "    - s4      tch, total cholesterol / HDL\n",
      "    - s5      ltg, possibly log of serum triglycerides level\n",
      "    - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "print(diabetes['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793f1356",
   "metadata": {},
   "source": [
    "나이, 성별, BMI, 평균 혈압, 6종 혈장 데이터를 442명의 당뇨병 환자로부터 얻은 10가지 데이터. Baseline으로부터 1년 후 병의 경과에 대한 양적 측정이 11번째 column으로 추가되어 있다(target).\n",
    "\n",
    "### 2-1. features\n",
    "\n",
    "- 나이: 측정시 나이\n",
    "- 성별\n",
    "- BMI: body max index\n",
    "- 평균 혈압: average blood pressure\n",
    "- 6 가지 혈장 데이터\n",
    "    - s1      tc, total serum cholesterol\n",
    "    - s2      ldl, low-density lipoproteins\n",
    "    - s3      hdl, high-density lipoproteins\n",
    "    - s4      tch, total cholesterol / HDL\n",
    "    - s5      ltg, possibly log of serum triglycerides level\n",
    "    - s6      glu, blood sugar level\n",
    "\n",
    "모든 10개의 attribute들은 평균으로 중심을 잡고 std * (n_sample)의 제곱근을 곱하여 스케일되어 있다.\n",
    "\n",
    "### 2-2. traget\n",
    "Baseline으로부터의 1년 후 병의 경과에 대한 양적 측정.\n",
    "\n",
    "### 3. 회귀/판단/분류\n",
    "양적 수치를 가지는 feature 들로 병의 진행도(양적 측정)를 target으로 하므로 회귀(Regression) 분석: [Scikit learn 설명](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes)\n",
    "\n",
    "### 4. 전체 데이터 수를 파악 / 판단-분류 분석이라면 타겟별 개수 출력\n",
    "442개\n",
    "\n",
    "### 5. feature와 target을 합쳐서 일괄로 보이는 dataframe 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930a927d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018114</td>\n",
       "      <td>0.044485</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017293</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046883</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044529</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081413</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004222</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017293 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081413  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  target  \n",
       "0   -0.002592  0.019907 -0.017646   151.0  \n",
       "1   -0.039493 -0.068332 -0.092204    75.0  \n",
       "2   -0.002592  0.002861 -0.025930   141.0  \n",
       "3    0.034309  0.022688 -0.009362   206.0  \n",
       "4   -0.002592 -0.031988 -0.046641   135.0  \n",
       "..        ...       ...       ...     ...  \n",
       "437 -0.002592  0.031193  0.007207   178.0  \n",
       "438  0.034309 -0.018114  0.044485   104.0  \n",
       "439 -0.011080 -0.046883  0.015491   132.0  \n",
       "440  0.026560  0.044529 -0.025930   220.0  \n",
       "441 -0.039493 -0.004222  0.003064    57.0  \n",
       "\n",
       "[442 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(diabetes['data'], columns=diabetes['feature_names'])\n",
    "df['target']=diabetes['target']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef85c5fe",
   "metadata": {},
   "source": [
    "### 6. Train/Test set을 70%, 30%로 적절하게 분리하는 소스를 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f5dfbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== train target ==============================\n",
      "target\n",
      "200.0     5\n",
      "85.0      4\n",
      "59.0      4\n",
      "202.0     4\n",
      "150.0     4\n",
      "         ..\n",
      "156.0     1\n",
      "31.0      1\n",
      "153.0     1\n",
      "152.0     1\n",
      "346.0     1\n",
      "Name: count, Length: 180, dtype: int64\n",
      "============================== test target ==============================\n",
      "target\n",
      "72.0      4\n",
      "88.0      3\n",
      "242.0     3\n",
      "109.0     3\n",
      "220.0     2\n",
      "         ..\n",
      "122.0     1\n",
      "121.0     1\n",
      "120.0     1\n",
      "118.0     1\n",
      "321.0     1\n",
      "Name: count, Length: 109, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "# 데이터 총 442 개, train: 309개, tes: 133개.\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "# 데이터프레임으로 정의\n",
    "df = pd.DataFrame(diabetes['data'], columns=diabetes['feature_names'])\n",
    "df['target']=diabetes['target']\n",
    "\n",
    "x_train = df.iloc[:309, :-1]\n",
    "x_test  = df.iloc[309:, :-1]\n",
    "y_train = df.iloc[:309, -1:]\n",
    "y_test  = df.iloc[309:, -1:]\n",
    "print(\"=\"*30, \"train target\", \"=\"*30)\n",
    "print(y_train.value_counts())\n",
    "print(\"=\"*30, \"test target\", \"=\"*30)\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585422b8",
   "metadata": {},
   "source": [
    "모든 attribute와 target이 양적인 수치이므로 속성을 count하는 것은 큰 의미가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.3. Optical Recognition of Handwritten Digits\n",
    "### 1. DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f027e81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 1797\n",
      ":Number of Attributes: 64\n",
      ":Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      ":Missing Attribute Values: None\n",
      ":Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      ":Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "print(digits['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb188f",
   "metadata": {},
   "source": [
    "- 데이터 객체 수: 1797개\n",
    "- attribute의 수 64개\n",
    "- attribute 정보: 8X8 격자 안의 픽셀 수로 0~ 16 까지를 가진다.\n",
    "- 빈 데이터 없음\n",
    "\n",
    "이 데이터는 손으로 작성한 숫자 데이터로 총 10개의 class(0 ~ 9)\n",
    "NIST에서 제공한 이미지를 normalize한 것. 43명의 수기를 추출하였으며, 30명이 training set이고 13명이 test set이다. 원본 이미지는 32X32 의 비트맵이고, 겹치지 않는 4X4 블록으로 나눠져있고 그 안의 픽셀 수를 센 것이 데이터이다. 이런 처리를 통해 8x8의 데이터가 생기며, 그 값이 0 ~ 16 까지가 되는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19fcc968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_0_0</th>\n",
       "      <th>pixel_0_1</th>\n",
       "      <th>pixel_0_2</th>\n",
       "      <th>pixel_0_3</th>\n",
       "      <th>pixel_0_4</th>\n",
       "      <th>pixel_0_5</th>\n",
       "      <th>pixel_0_6</th>\n",
       "      <th>pixel_0_7</th>\n",
       "      <th>pixel_1_0</th>\n",
       "      <th>pixel_1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_6_7</th>\n",
       "      <th>pixel_7_0</th>\n",
       "      <th>pixel_7_1</th>\n",
       "      <th>pixel_7_2</th>\n",
       "      <th>pixel_7_3</th>\n",
       "      <th>pixel_7_4</th>\n",
       "      <th>pixel_7_5</th>\n",
       "      <th>pixel_7_6</th>\n",
       "      <th>pixel_7_7</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1797 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
       "0           0.0        0.0        5.0       13.0        9.0        1.0   \n",
       "1           0.0        0.0        0.0       12.0       13.0        5.0   \n",
       "2           0.0        0.0        0.0        4.0       15.0       12.0   \n",
       "3           0.0        0.0        7.0       15.0       13.0        1.0   \n",
       "4           0.0        0.0        0.0        1.0       11.0        0.0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1792        0.0        0.0        4.0       10.0       13.0        6.0   \n",
       "1793        0.0        0.0        6.0       16.0       13.0       11.0   \n",
       "1794        0.0        0.0        1.0       11.0       15.0        1.0   \n",
       "1795        0.0        0.0        2.0       10.0        7.0        0.0   \n",
       "1796        0.0        0.0       10.0       14.0        8.0        1.0   \n",
       "\n",
       "      pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_7  pixel_7_0  \\\n",
       "0           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "2           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "3           0.0        0.0        0.0        8.0  ...        0.0        0.0   \n",
       "4           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "1792        0.0        0.0        0.0        1.0  ...        0.0        0.0   \n",
       "1793        1.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1794        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1795        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1796        0.0        0.0        0.0        2.0  ...        0.0        0.0   \n",
       "\n",
       "      pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  pixel_7_6  \\\n",
       "0           0.0        6.0       13.0       10.0        0.0        0.0   \n",
       "1           0.0        0.0       11.0       16.0       10.0        0.0   \n",
       "2           0.0        0.0        3.0       11.0       16.0        9.0   \n",
       "3           0.0        7.0       13.0       13.0        9.0        0.0   \n",
       "4           0.0        0.0        2.0       16.0        4.0        0.0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1792        0.0        2.0       14.0       15.0        9.0        0.0   \n",
       "1793        0.0        6.0       16.0       14.0        6.0        0.0   \n",
       "1794        0.0        2.0        9.0       13.0        6.0        0.0   \n",
       "1795        0.0        5.0       12.0       16.0       12.0        0.0   \n",
       "1796        1.0        8.0       12.0       14.0       12.0        1.0   \n",
       "\n",
       "      pixel_7_7  target  \n",
       "0           0.0       0  \n",
       "1           0.0       1  \n",
       "2           0.0       2  \n",
       "3           0.0       3  \n",
       "4           0.0       4  \n",
       "...         ...     ...  \n",
       "1792        0.0       9  \n",
       "1793        0.0       0  \n",
       "1794        0.0       8  \n",
       "1795        0.0       9  \n",
       "1796        0.0       8  \n",
       "\n",
       "[1797 rows x 65 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(digits['data'], columns=digits['feature_names'])\n",
    "df['target']=digits['target']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d70410",
   "metadata": {},
   "source": [
    "### 2. Feature & Target\n",
    "- Features: 8X8 격자에 0 ~ 16의 데이터\n",
    "- Target: 각 8X8(64개)의 데이터가 하나의 숫자를 나타냄\n",
    "\n",
    "### 3. 회귀/판단/분류\n",
    "각 비트맵 이미지가 어떤 숫자를 나타내는지 분류하는 분석임.\n",
    "\n",
    "### 4. 전체 데이터 수와 타겟별 개수\n",
    "총 1797개의 데이터를 가지고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f3490e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "3         183\n",
      "1         182\n",
      "5         182\n",
      "4         181\n",
      "6         181\n",
      "9         180\n",
      "7         179\n",
      "0         178\n",
      "2         177\n",
      "8         174\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "# 세트는 총 150개, 120개는 train, 나머지 30개는 test로 설정하려고 한다.\n",
    "\n",
    "digits = load_digits()\n",
    "# 데이터프레임으로 정의\n",
    "df = pd.DataFrame(digits['data'], columns=digits['feature_names'])\n",
    "df['target']=digits['target']\n",
    "\n",
    "y  = df.iloc[:, -1:]\n",
    "\n",
    "print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89a51c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_0_0</th>\n",
       "      <th>pixel_0_1</th>\n",
       "      <th>pixel_0_2</th>\n",
       "      <th>pixel_0_3</th>\n",
       "      <th>pixel_0_4</th>\n",
       "      <th>pixel_0_5</th>\n",
       "      <th>pixel_0_6</th>\n",
       "      <th>pixel_0_7</th>\n",
       "      <th>pixel_1_0</th>\n",
       "      <th>pixel_1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_6_7</th>\n",
       "      <th>pixel_7_0</th>\n",
       "      <th>pixel_7_1</th>\n",
       "      <th>pixel_7_2</th>\n",
       "      <th>pixel_7_3</th>\n",
       "      <th>pixel_7_4</th>\n",
       "      <th>pixel_7_5</th>\n",
       "      <th>pixel_7_6</th>\n",
       "      <th>pixel_7_7</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1797 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
       "0           0.0        0.0        5.0       13.0        9.0        1.0   \n",
       "1           0.0        0.0        0.0       12.0       13.0        5.0   \n",
       "2           0.0        0.0        0.0        4.0       15.0       12.0   \n",
       "3           0.0        0.0        7.0       15.0       13.0        1.0   \n",
       "4           0.0        0.0        0.0        1.0       11.0        0.0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1792        0.0        0.0        4.0       10.0       13.0        6.0   \n",
       "1793        0.0        0.0        6.0       16.0       13.0       11.0   \n",
       "1794        0.0        0.0        1.0       11.0       15.0        1.0   \n",
       "1795        0.0        0.0        2.0       10.0        7.0        0.0   \n",
       "1796        0.0        0.0       10.0       14.0        8.0        1.0   \n",
       "\n",
       "      pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_7  pixel_7_0  \\\n",
       "0           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "2           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "3           0.0        0.0        0.0        8.0  ...        0.0        0.0   \n",
       "4           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "1792        0.0        0.0        0.0        1.0  ...        0.0        0.0   \n",
       "1793        1.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1794        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1795        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1796        0.0        0.0        0.0        2.0  ...        0.0        0.0   \n",
       "\n",
       "      pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  pixel_7_6  \\\n",
       "0           0.0        6.0       13.0       10.0        0.0        0.0   \n",
       "1           0.0        0.0       11.0       16.0       10.0        0.0   \n",
       "2           0.0        0.0        3.0       11.0       16.0        9.0   \n",
       "3           0.0        7.0       13.0       13.0        9.0        0.0   \n",
       "4           0.0        0.0        2.0       16.0        4.0        0.0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1792        0.0        2.0       14.0       15.0        9.0        0.0   \n",
       "1793        0.0        6.0       16.0       14.0        6.0        0.0   \n",
       "1794        0.0        2.0        9.0       13.0        6.0        0.0   \n",
       "1795        0.0        5.0       12.0       16.0       12.0        0.0   \n",
       "1796        1.0        8.0       12.0       14.0       12.0        1.0   \n",
       "\n",
       "      pixel_7_7  target  \n",
       "0           0.0       0  \n",
       "1           0.0       1  \n",
       "2           0.0       2  \n",
       "3           0.0       3  \n",
       "4           0.0       4  \n",
       "...         ...     ...  \n",
       "1792        0.0       9  \n",
       "1793        0.0       0  \n",
       "1794        0.0       8  \n",
       "1795        0.0       9  \n",
       "1796        0.0       8  \n",
       "\n",
       "[1797 rows x 65 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(digits['data'], columns=digits['feature_names'])\n",
    "df['target']=digits['target']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239054aa",
   "metadata": {},
   "source": [
    "### 6. train/test set을 적절하게 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efd47197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== train target ==============================\n",
      "target\n",
      "3         130\n",
      "1         129\n",
      "6         127\n",
      "5         126\n",
      "0         125\n",
      "4         125\n",
      "7         125\n",
      "9         125\n",
      "2         124\n",
      "8         122\n",
      "Name: count, dtype: int64\n",
      "============================== test target ==============================\n",
      "target\n",
      "4         56\n",
      "5         56\n",
      "9         55\n",
      "6         54\n",
      "7         54\n",
      "0         53\n",
      "1         53\n",
      "2         53\n",
      "3         53\n",
      "8         52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "# 세트는 총 1797개, 1258개는 train, 나머지 499개는 test로 설정하려고 한다.\n",
    "\n",
    "digits = load_digits()\n",
    "# 데이터프레임으로 정의\n",
    "df = pd.DataFrame(digits['data'], columns=digits['feature_names'])\n",
    "df['target']=digits['target']\n",
    "\n",
    "x_train = df.iloc[:1258, :-1]\n",
    "x_test  = df.iloc[1258:, :-1]\n",
    "y_train = df.iloc[:1258, -1:]\n",
    "y_test  = df.iloc[1258:, -1:]\n",
    "print(\"=\"*30, \"train target\", \"=\"*30)\n",
    "# print(\"x_train: \", x_train)\n",
    "# print(\"y_train: \", y_train)\n",
    "print(y_train.value_counts())\n",
    "print(\"=\"*30, \"test target\", \"=\"*30)\n",
    "# print(\"x_test: \", x_test)\n",
    "# print(\"y_test: \", y_test)\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353cdc9b",
   "metadata": {},
   "source": [
    "## 7.1.4. Linnerud\n",
    "### 1. DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4bb7d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _linnerrud_dataset:\n",
      "\n",
      "Linnerrud dataset\n",
      "-----------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 20\n",
      ":Number of Attributes: 3\n",
      ":Missing Attribute Values: None\n",
      "\n",
      "The Linnerud dataset is a multi-output regression dataset. It consists of three\n",
      "exercise (data) and three physiological (target) variables collected from\n",
      "twenty middle-aged men in a fitness club:\n",
      "\n",
      "- *physiological* - CSV containing 20 observations on 3 physiological variables:\n",
      "   Weight, Waist and Pulse.\n",
      "- *exercise* - CSV containing 20 observations on 3 exercise variables:\n",
      "   Chins, Situps and Jumps.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "   * Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:\n",
      "     Editions Technic.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_linnerud\n",
    "\n",
    "linnerud = load_linnerud()\n",
    "print(linnerud['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4300b2",
   "metadata": {},
   "source": [
    "Linnerrud 데이터셋은 다중 출력 회귀에 대한 데이터셋이다. Fitness club의 20대를 대상으로 얻은 세 운동에 대한 데이터-턱걸이, 앉았다 일어서기, 점프-와 세 생리학적 데이터-몸무게, 허리둘레, 맥박-들을 target으로 하여 이루어져있다.\n",
    "\n",
    "### 2. Feature와 Target\n",
    "- Feature: 턱걸이 / 앉았다 일어서기 / 점프\n",
    "- Target: 몸무게 / 허리둘레 / 맥박"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "571f481f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chins', 'Situps', 'Jumps']\n",
      "['Weight', 'Waist', 'Pulse']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_linnerud\n",
    "\n",
    "linnerud = load_linnerud()\n",
    "\n",
    "print(linnerud['feature_names'])\n",
    "print(linnerud['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9513d2a1",
   "metadata": {},
   "source": [
    "### 3. 회귀, 판단, 분류\n",
    "이 분석은 회귀 분석으로, 여러 변수를 가지고 각 target의 수치를 예측하는 데이터셋이다.\n",
    "\n",
    "### 4. 전체 데이터 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e53a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5. 162.  60.]\n",
      " [  2. 110.  60.]\n",
      " [ 12. 101. 101.]\n",
      " [ 12. 105.  37.]\n",
      " [ 13. 155.  58.]\n",
      " [  4. 101.  42.]\n",
      " [  8. 101.  38.]\n",
      " [  6. 125.  40.]\n",
      " [ 15. 200.  40.]\n",
      " [ 17. 251. 250.]\n",
      " [ 17. 120.  38.]\n",
      " [ 13. 210. 115.]\n",
      " [ 14. 215. 105.]\n",
      " [  1.  50.  50.]\n",
      " [  6.  70.  31.]\n",
      " [ 12. 210. 120.]\n",
      " [  4.  60.  25.]\n",
      " [ 11. 230.  80.]\n",
      " [ 15. 225.  73.]\n",
      " [  2. 110.  43.]]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(linnerud['data'])\n",
    "print(len(linnerud['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f0c075",
   "metadata": {},
   "source": [
    "### 5. Feature와 target을 합친 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0674b7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[  5., 162.,  60.],\n",
       "        [  2., 110.,  60.],\n",
       "        [ 12., 101., 101.],\n",
       "        [ 12., 105.,  37.],\n",
       "        [ 13., 155.,  58.],\n",
       "        [  4., 101.,  42.],\n",
       "        [  8., 101.,  38.],\n",
       "        [  6., 125.,  40.],\n",
       "        [ 15., 200.,  40.],\n",
       "        [ 17., 251., 250.],\n",
       "        [ 17., 120.,  38.],\n",
       "        [ 13., 210., 115.],\n",
       "        [ 14., 215., 105.],\n",
       "        [  1.,  50.,  50.],\n",
       "        [  6.,  70.,  31.],\n",
       "        [ 12., 210., 120.],\n",
       "        [  4.,  60.,  25.],\n",
       "        [ 11., 230.,  80.],\n",
       "        [ 15., 225.,  73.],\n",
       "        [  2., 110.,  43.]]),\n",
       " 'feature_names': ['Chins', 'Situps', 'Jumps'],\n",
       " 'target': array([[191.,  36.,  50.],\n",
       "        [189.,  37.,  52.],\n",
       "        [193.,  38.,  58.],\n",
       "        [162.,  35.,  62.],\n",
       "        [189.,  35.,  46.],\n",
       "        [182.,  36.,  56.],\n",
       "        [211.,  38.,  56.],\n",
       "        [167.,  34.,  60.],\n",
       "        [176.,  31.,  74.],\n",
       "        [154.,  33.,  56.],\n",
       "        [169.,  34.,  50.],\n",
       "        [166.,  33.,  52.],\n",
       "        [154.,  34.,  64.],\n",
       "        [247.,  46.,  50.],\n",
       "        [193.,  36.,  46.],\n",
       "        [202.,  37.,  62.],\n",
       "        [176.,  37.,  54.],\n",
       "        [157.,  32.,  52.],\n",
       "        [156.,  33.,  54.],\n",
       "        [138.,  33.,  68.]]),\n",
       " 'target_names': ['Weight', 'Waist', 'Pulse'],\n",
       " 'frame': None,\n",
       " 'DESCR': '.. _linnerrud_dataset:\\n\\nLinnerrud dataset\\n-----------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 20\\n:Number of Attributes: 3\\n:Missing Attribute Values: None\\n\\nThe Linnerud dataset is a multi-output regression dataset. It consists of three\\nexercise (data) and three physiological (target) variables collected from\\ntwenty middle-aged men in a fitness club:\\n\\n- *physiological* - CSV containing 20 observations on 3 physiological variables:\\n   Weight, Waist and Pulse.\\n- *exercise* - CSV containing 20 observations on 3 exercise variables:\\n   Chins, Situps and Jumps.\\n\\n.. dropdown:: References\\n\\n   * Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:\\n     Editions Technic.\\n',\n",
       " 'data_filename': 'linnerud_exercise.csv',\n",
       " 'target_filename': 'linnerud_physiological.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linnerud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eaa1caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chins</th>\n",
       "      <th>Situps</th>\n",
       "      <th>Jumps</th>\n",
       "      <th>target: Weigth</th>\n",
       "      <th>target: Waist</th>\n",
       "      <th>target: Pulse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Chins  Situps  Jumps  target: Weigth  target: Waist  target: Pulse\n",
       "0     5.0   162.0   60.0           191.0           36.0           50.0\n",
       "1     2.0   110.0   60.0           189.0           37.0           52.0\n",
       "2    12.0   101.0  101.0           193.0           38.0           58.0\n",
       "3    12.0   105.0   37.0           162.0           35.0           62.0\n",
       "4    13.0   155.0   58.0           189.0           35.0           46.0\n",
       "5     4.0   101.0   42.0           182.0           36.0           56.0\n",
       "6     8.0   101.0   38.0           211.0           38.0           56.0\n",
       "7     6.0   125.0   40.0           167.0           34.0           60.0\n",
       "8    15.0   200.0   40.0           176.0           31.0           74.0\n",
       "9    17.0   251.0  250.0           154.0           33.0           56.0\n",
       "10   17.0   120.0   38.0           169.0           34.0           50.0\n",
       "11   13.0   210.0  115.0           166.0           33.0           52.0\n",
       "12   14.0   215.0  105.0           154.0           34.0           64.0\n",
       "13    1.0    50.0   50.0           247.0           46.0           50.0\n",
       "14    6.0    70.0   31.0           193.0           36.0           46.0\n",
       "15   12.0   210.0  120.0           202.0           37.0           62.0\n",
       "16    4.0    60.0   25.0           176.0           37.0           54.0\n",
       "17   11.0   230.0   80.0           157.0           32.0           52.0\n",
       "18   15.0   225.0   73.0           156.0           33.0           54.0\n",
       "19    2.0   110.0   43.0           138.0           33.0           68.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(linnerud['data'], columns=linnerud['feature_names'])\n",
    "df['target: Weigth']=linnerud['target'][:, 0]\n",
    "df['target: Waist']=linnerud['target'][:, 1]\n",
    "df['target: Pulse']=linnerud['target'][:, 2]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dce348",
   "metadata": {},
   "source": [
    "### 6. Train/test set 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77b525ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== train target ==============================\n",
      "target: Weigth\n",
      "189.0    2\n",
      "154.0    2\n",
      "191.0    1\n",
      "193.0    1\n",
      "162.0    1\n",
      "182.0    1\n",
      "211.0    1\n",
      "167.0    1\n",
      "176.0    1\n",
      "169.0    1\n",
      "166.0    1\n",
      "247.0    1\n",
      "Name: count, dtype: int64\n",
      "target: Waist\n",
      "34.0    3\n",
      "36.0    2\n",
      "38.0    2\n",
      "35.0    2\n",
      "33.0    2\n",
      "37.0    1\n",
      "31.0    1\n",
      "46.0    1\n",
      "Name: count, dtype: int64\n",
      "target: Pulse\n",
      "50.0             3\n",
      "56.0             3\n",
      "52.0             2\n",
      "46.0             1\n",
      "58.0             1\n",
      "60.0             1\n",
      "62.0             1\n",
      "64.0             1\n",
      "74.0             1\n",
      "Name: count, dtype: int64\n",
      "============================== test target ==============================\n",
      "target: Weigth\n",
      "193.0    1\n",
      "202.0    1\n",
      "176.0    1\n",
      "157.0    1\n",
      "156.0    1\n",
      "138.0    1\n",
      "Name: count, dtype: int64\n",
      "target: Waist\n",
      "37.0    2\n",
      "33.0    2\n",
      "36.0    1\n",
      "32.0    1\n",
      "Name: count, dtype: int64\n",
      "target: Pulse\n",
      "54.0             2\n",
      "46.0             1\n",
      "52.0             1\n",
      "62.0             1\n",
      "68.0             1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_linnerud\n",
    "# 세트는 총 20개, train 데이터는 14개, 테스트데이터는 6개.\n",
    "\n",
    "linnerud = load_linnerud()\n",
    "# 데이터프레임으로 정의\n",
    "df = pd.DataFrame(linnerud['data'], columns=linnerud['feature_names'])\n",
    "df['target: Weigth']=linnerud['target'][:, 0]\n",
    "df['target: Waist']=linnerud['target'][:, 1]\n",
    "df['target: Pulse']=linnerud['target'][:, 2]\n",
    "\n",
    "x_train = df.iloc[:14, :-1]\n",
    "x_test  = df.iloc[14:, :-1]\n",
    "y_train_Weights = df.iloc[:14, -3]\n",
    "y_train_Waist = df.iloc[:14, -2]\n",
    "y_train_Pulse = df.iloc[:14, -1:]\n",
    "y_test_Weights  = df.iloc[14:, -3]\n",
    "y_test_Waist  = df.iloc[14:, -2]\n",
    "y_test_Pulse  = df.iloc[14:, -1:]\n",
    "print(\"=\"*30, \"train target\", \"=\"*30)\n",
    "# print(\"x_train: \", x_train)\n",
    "# print(\"y_train: \", y_train)\n",
    "print(y_train_Weights.value_counts())\n",
    "print(y_train_Waist.value_counts())\n",
    "print(y_train_Pulse.value_counts())\n",
    "print(\"=\"*30, \"test target\", \"=\"*30)\n",
    "# print(\"x_test: \", x_test)\n",
    "# print(\"y_test: \", y_test)\n",
    "print(y_test_Weights.value_counts())\n",
    "print(y_test_Waist.value_counts())\n",
    "print(y_test_Pulse.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929e00e0",
   "metadata": {},
   "source": [
    "## 7.1.5 Wine Recognition\n",
    "### 1. DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "755913ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 178\n",
      ":Number of Attributes: 13 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - Alcohol\n",
      "    - Malic acid\n",
      "    - Ash\n",
      "    - Alcalinity of ash\n",
      "    - Magnesium\n",
      "    - Total phenols\n",
      "    - Flavanoids\n",
      "    - Nonflavanoid phenols\n",
      "    - Proanthocyanins\n",
      "    - Color intensity\n",
      "    - Hue\n",
      "    - OD280/OD315 of diluted wines\n",
      "    - Proline\n",
      "    - class:\n",
      "        - class_0\n",
      "        - class_1\n",
      "        - class_2\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============================= ==== ===== ======= =====\n",
      "                                Min   Max   Mean     SD\n",
      "============================= ==== ===== ======= =====\n",
      "Alcohol:                      11.0  14.8    13.0   0.8\n",
      "Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "Ash:                          1.36  3.23    2.36  0.27\n",
      "Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "Magnesium:                    70.0 162.0    99.7  14.3\n",
      "Total Phenols:                0.98  3.88    2.29  0.63\n",
      "Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "Hue:                          0.48  1.71    0.96  0.23\n",
      "OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "Proline:                       278  1680     746   315\n",
      "============================= ==== ===== ======= =====\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners:\n",
      "\n",
      "Forina, M. et al, PARVUS -\n",
      "An Extendible Package for Data Exploration, Classification and Correlation.\n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "    (1) S. Aeberhard, D. Coomans and O. de Vel,\n",
      "    Comparison of Classifiers in High Dimensional Settings,\n",
      "    Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of\n",
      "    Mathematics and Statistics, James Cook University of North Queensland.\n",
      "    (Also submitted to Technometrics).\n",
      "\n",
      "    The data was used with many others for comparing various\n",
      "    classifiers. The classes are separable, though only RDA\n",
      "    has achieved 100% correct classification.\n",
      "    (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data))\n",
      "    (All results using the leave-one-out technique)\n",
      "\n",
      "    (2) S. Aeberhard, D. Coomans and O. de Vel,\n",
      "    \"THE CLASSIFICATION PERFORMANCE OF RDA\"\n",
      "    Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of\n",
      "    Mathematics and Statistics, James Cook University of North Queensland.\n",
      "    (Also submitted to Journal of Chemometrics).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "print(wine['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac315d",
   "metadata": {},
   "source": [
    "와인 분류 데이터셋. 총 178개의 데이터를 가지고 있으며, 13개의 숫자 데이터를 가지고 있고 등급을 target으로 가지고 있음. 데이터는 화학 분석의 결과로, 이탈리아의 같은 지역에서 다른 경작자에 의해 재배된 포도로 만들어졌다.\n",
    "\n",
    "### 2. Feature과 Target\n",
    "- Feature: 모두 number 형태의 데이터들임. 총 13 가지 항목. Alcohol / Malic acid / Ash / Alcalinity of Ash / Magnesium / Total phenols / Flavanoids / NonFlavanoid phenols / Proanthocyanis / Color intensity / Hue / OD280/OD315 비 / Proline\n",
    "- Target: 와인의 클래스. class_0 (59), class_1 (71), class_2 (48)\n",
    "\n",
    "### 3. 회귀 / 판단 / 분류\n",
    "분류 분석임.\n",
    "\n",
    "### 4. 전체 데이터 수와 타겟별 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e07f97e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 수:  178\n",
      "target\n",
      "1    71\n",
      "0    59\n",
      "2    48\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "\n",
    "print('전체 데이터 수: ', len(wine['data']))\n",
    "df = pd.DataFrame(wine['data'], columns=wine['feature_names'])\n",
    "df['target']=wine['target']\n",
    "\n",
    "print(df.iloc[:, -1].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb45e2c",
   "metadata": {},
   "source": [
    "### 5. Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7798ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  target  \n",
       "0                            3.92   1065.0       0  \n",
       "1                            3.40   1050.0       0  \n",
       "2                            3.17   1185.0       0  \n",
       "3                            3.45   1480.0       0  \n",
       "4                            2.93    735.0       0  \n",
       "..                            ...      ...     ...  \n",
       "173                          1.74    740.0       2  \n",
       "174                          1.56    750.0       2  \n",
       "175                          1.56    835.0       2  \n",
       "176                          1.62    840.0       2  \n",
       "177                          1.60    560.0       2  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(wine['data'], columns=wine['feature_names'])\n",
    "df['target']=wine['target']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e76911f",
   "metadata": {},
   "source": [
    "### 7. Test와 Train을 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95895c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== train target ==============================\n",
      "target\n",
      "1         57\n",
      "0         37\n",
      "2         31\n",
      "Name: count, dtype: int64\n",
      "============================== test target ==============================\n",
      "target\n",
      "0         22\n",
      "2         17\n",
      "1         14\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "# 데이터 총 178 개, train: 125, test: 53개.\n",
    "\n",
    "wine = load_wine()\n",
    "# 데이터프레임으로 정의\n",
    "df = pd.DataFrame(wine['data'], columns=wine['feature_names'])\n",
    "df['target']=wine['target']\n",
    "\n",
    "df_shuffled = df.iloc[np.random.permutation(df.index)].reset_index(drop=True)\n",
    "\n",
    "x_train = df_shuffled.iloc[:125, :-1]\n",
    "x_test  = df_shuffled.iloc[125:, :-1]\n",
    "y_train = df_shuffled.iloc[:125, -1:]\n",
    "y_test  = df_shuffled.iloc[125:, -1:]\n",
    "print(\"=\"*30, \"train target\", \"=\"*30)\n",
    "# print(\"x_train: \", x_train)\n",
    "# print(\"y_train: \", y_train)\n",
    "print(y_train.value_counts())\n",
    "print(\"=\"*30, \"test target\", \"=\"*30)\n",
    "# print(\"x_test: \", x_test)\n",
    "# print(\"y_test: \", y_test)\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44889d06",
   "metadata": {},
   "source": [
    "## 7.1.6. Breast Cancer Wisconsin\n",
    "### 1. DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "911e6b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 569\n",
      "\n",
      ":Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      ":Attribute Information:\n",
      "    - radius (mean of distances from center to points on the perimeter)\n",
      "    - texture (standard deviation of gray-scale values)\n",
      "    - perimeter\n",
      "    - area\n",
      "    - smoothness (local variation in radius lengths)\n",
      "    - compactness (perimeter^2 / area - 1.0)\n",
      "    - concavity (severity of concave portions of the contour)\n",
      "    - concave points (number of concave portions of the contour)\n",
      "    - symmetry\n",
      "    - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "    The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "    worst/largest values) of these features were computed for each image,\n",
      "    resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "    10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "    - class:\n",
      "            - WDBC-Malignant\n",
      "            - WDBC-Benign\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "===================================== ====== ======\n",
      "                                        Min    Max\n",
      "===================================== ====== ======\n",
      "radius (mean):                        6.981  28.11\n",
      "texture (mean):                       9.71   39.28\n",
      "perimeter (mean):                     43.79  188.5\n",
      "area (mean):                          143.5  2501.0\n",
      "smoothness (mean):                    0.053  0.163\n",
      "compactness (mean):                   0.019  0.345\n",
      "concavity (mean):                     0.0    0.427\n",
      "concave points (mean):                0.0    0.201\n",
      "symmetry (mean):                      0.106  0.304\n",
      "fractal dimension (mean):             0.05   0.097\n",
      "radius (standard error):              0.112  2.873\n",
      "texture (standard error):             0.36   4.885\n",
      "perimeter (standard error):           0.757  21.98\n",
      "area (standard error):                6.802  542.2\n",
      "smoothness (standard error):          0.002  0.031\n",
      "compactness (standard error):         0.002  0.135\n",
      "concavity (standard error):           0.0    0.396\n",
      "concave points (standard error):      0.0    0.053\n",
      "symmetry (standard error):            0.008  0.079\n",
      "fractal dimension (standard error):   0.001  0.03\n",
      "radius (worst):                       7.93   36.04\n",
      "texture (worst):                      12.02  49.54\n",
      "perimeter (worst):                    50.41  251.2\n",
      "area (worst):                         185.2  4254.0\n",
      "smoothness (worst):                   0.071  0.223\n",
      "compactness (worst):                  0.027  1.058\n",
      "concavity (worst):                    0.0    1.252\n",
      "concave points (worst):               0.0    0.291\n",
      "symmetry (worst):                     0.156  0.664\n",
      "fractal dimension (worst):            0.055  0.208\n",
      "===================================== ====== ======\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      ":Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      ":Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      ":Donor: Nick Street\n",
      "\n",
      ":Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\n",
      "    for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\n",
      "    Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "    San Jose, CA, 1993.\n",
      "  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\n",
      "    prognosis via linear programming. Operations Research, 43(4), pages 570-577,\n",
      "    July-August 1995.\n",
      "  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "    to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\n",
      "    163-171.\n",
      "\n",
      "['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast = load_breast_cancer()\n",
    "\n",
    "print(breast['DESCR'])\n",
    "print(breast['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ecedaf",
   "metadata": {},
   "source": [
    "유방암에 대한 진단 데이터. 총 데이터 수는 569이며, 예측에 사용되는 30 개의 속성임.\n",
    "\n",
    "### 2. Feature 와 Target\n",
    "- Feature: 반지름 / 질감 / 둘레 / 영역 / 부드러움 / 밀도감 / 오목함 / ... / 대칭 / 프랙탈 형태\n",
    "- Target: 두 가지로 구분된다: malignant(악성) / benign(양성)\n",
    "\n",
    "### 3. 회귀 / 판단 / 분류\n",
    "판단 분석임.\n",
    "\n",
    "### 4. 전체 데이터 수와 타겟별 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27d21608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 갯수:  569\n",
      "target\n",
      "1         357\n",
      "0         212\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "# 세트는 총 150개, 120개는 train, 나머지 30개는 test로 설정하려고 한다.\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "# 데이터프레임으로 정의\n",
    "df = pd.DataFrame(breast_cancer['data'], columns=breast_cancer['feature_names'])\n",
    "df['target']=breast_cancer['target']\n",
    "\n",
    "y  = df.iloc[:, -1:]\n",
    "\n",
    "print('전체 데이터 갯수: ', len(y))\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3621bd",
   "metadata": {},
   "source": [
    "### 5. Feature와 Target을 합친 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0b519e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(breast_cancer['data'], columns=breast_cancer['feature_names'])\n",
    "df['target']=breast_cancer['target']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e108ec",
   "metadata": {},
   "source": [
    "### 6. Train/Test의 적절한 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa00357f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== train target ==============================\n",
      "target\n",
      "1         252\n",
      "0         147\n",
      "Name: count, dtype: int64\n",
      "============================== test target ==============================\n",
      "target\n",
      "1         105\n",
      "0          65\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "# 세트는 총 569개, 399개는 train, 나머지 170개는 test로 설정하려고 한다.\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "# 데이터프레임으로 정의\n",
    "df = pd.DataFrame(breast_cancer['data'], columns=breast_cancer['feature_names'])\n",
    "df['target']=breast_cancer['target']\n",
    "\n",
    "df_shuffled = df.iloc[np.random.permutation(df.index)].reset_index(drop=True)\n",
    "\n",
    "x_train = df_shuffled.iloc[:399, :-1]\n",
    "x_test  = df_shuffled.iloc[399:, :-1]\n",
    "y_train = df_shuffled.iloc[:399, -1:]\n",
    "y_test  = df_shuffled.iloc[399:, -1:]\n",
    "print(\"=\"*30, \"train target\", \"=\"*30)\n",
    "# print(\"x_train: \", x_train)\n",
    "# print(\"y_train: \", y_train)\n",
    "print(y_train.value_counts())\n",
    "print(\"=\"*30, \"test target\", \"=\"*30)\n",
    "# print(\"x_test: \", x_test)\n",
    "# print(\"y_test: \", y_test)\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92562f5a-f240-4759-a661-d3fbc29cd361",
   "metadata": {},
   "source": [
    "## Linear Regerssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5bbd176-f2e5-4a13-8110-864eb82eff2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([2, 4, 6, 8])\n",
    "y_data = np.array([81, 93, 91, 97])\n",
    "\n",
    "plt.scatter(x_data, y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d198fe-c96a-4b76-82e3-39d13f11c53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기울기 a:  [[2.3]]  y절편 b:  [79.]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x_data = np.array([2, 4, 6, 8])\n",
    "y_data = np.array([81, 93, 91, 97])\n",
    "\n",
    "# sklearn에 있는 LinearRegression 연산으로 데이터에 대한 Linear Regression한 결과를 저장한다.\n",
    "LR = LinearRegression().fit(x_data.reshape(-1, 1), y_data.reshape(-1, 1)) \n",
    "\n",
    "print(\"기울기 a: \", LR.coef_, \" y절편 b: \", LR.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8b1ce66-0f36-4085-956a-8707d1195223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([2, 4, 6, 8])\n",
    "y_data = np.array([81, 93, 91, 97])\n",
    "plt.scatter(x_data, y_data)\n",
    "\n",
    "a = 2.3; b=79.\n",
    "\n",
    "start_x = x_data[0]\n",
    "start_y = a*start_x+b\n",
    "\n",
    "end_x = x_data[-1]\n",
    "end_y = a*end_x+b\n",
    "\n",
    "# Linear Regression으로 만든 기울기와 절편 값으로 첫 x 데이터 / 마지막 x 데이터로 두개의 점을 만들어 연결하였다.\n",
    "plt.plot([start_x, end_x], [start_y, end_y]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "263bd441-ad51-4f80-9ca7-d3cbe7011dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.5 시간 공부하였을 대 예상 득점:  98.55\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "a=2.3\n",
    "b=79.\n",
    "\n",
    "x=8.5\n",
    "y=a*x+b\n",
    "\n",
    "print(x, \"시간 공부하였을 대 예상 득점: \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8156db97-6c29-450a-a053-40ed5751125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\ty\t예측값\t에러\t에러절대값\t에러제곱\n",
      "2\t81\t83.60\t2.60\t2.60\t\t6.76\n",
      "4\t93\t88.20\t-4.80\t4.80\t\t23.04\n",
      "6\t91\t92.80\t1.80\t1.80\t\t3.24\n",
      "8\t97\t97.40\t0.40\t0.40\t\t0.16\n",
      "==============================\n",
      "에러합: 0.00 에러평균:0.00\n",
      "에러절대값합: 9.60 에러절대값평균:2.40\n",
      "에러제곱합: 33.20 에러제곱평균:8.30\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "x_data = np.array([2, 4, 6, 8])\n",
    "y_data = np.array([81, 93, 91, 97])\n",
    "\n",
    "err_sum = 0\n",
    "err_mae_sum = 0\n",
    "err_mse_sum = 0\n",
    "\n",
    "print(\"x\\ty\\t예측값\\t에러\\t에러절대값\\t에러제곱\")\n",
    "for i in range(len(x_data)):\n",
    "    pred=(2.3*x_data[i]+79)\n",
    "    err= pred - y_data[i]\n",
    "    err_mae = np.abs(err)\n",
    "    err_mse = np.power(err, 2)\n",
    "    print(\"%d\\t%d\\t%.2f\\t%.2f\\t%.2f\\t\\t%.2f\"%(x_data[i], y_data[i], pred, err, err_mae, err_mse))\n",
    "    err_sum = err_sum + err\n",
    "    err_mae_sum = err_mae_sum + err_mae\n",
    "    err_mse_sum = err_mse_sum + err_mse\n",
    "print(\"=\"*30)\n",
    "print(\"에러합: %.2f 에러평균:%.2f\"%(err_sum, err_sum/len(x_data)))\n",
    "print(\"에러절대값합: %.2f 에러절대값평균:%.2f\"%(err_mae_sum, err_mae_sum/len(x_data)))\n",
    "print(\"에러제곱합: %.2f 에러제곱평균:%.2f\"%(err_mse_sum, err_mse_sum/len(x_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f49ca251-6909-4586-a526-bb0f76eb17c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file write done\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "x_data = np.array([2, 4, 6, 8])\n",
    "y_data = np.array([81, 93, 91, 97])\n",
    "\n",
    "## 초기값\n",
    "a=1.0;a_rate=0.1\n",
    "b=1.0;b_rate=0.1\n",
    "f=open(\"./data/LR_mse.csv\", \"w\")\n",
    "\n",
    "while True:\n",
    "    while True:\n",
    "        y_pred = a * x_data + b\n",
    "        error = np.power((y_pred - y_data), 2).mean() # mse\n",
    "\n",
    "        ldata = str(a) + \",\" + str(b) + \",\" + str(error) + \"\\n\"\n",
    "        f.write(ldata)\n",
    "\n",
    "        b = b + b_rate\n",
    "        if b > 100:\n",
    "            break\n",
    "\n",
    "    a = a + a_rate\n",
    "    b = 1.0\n",
    "    if a > 100: break\n",
    "\n",
    "f.close()\n",
    "print(\"file write done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593d18eb",
   "metadata": {},
   "source": [
    "## 경사하강법(Gradient Descent)\n",
    "\n",
    "선형 회귀의 오류 검출 방법인 mse 혹은 mae로 산출한 손실 함수(Loss function)의 오류값을 줄이므로써 예측의 정확성을 늘리는 예측 방법이다.\n",
    "- MSe: Mean Square Error 제곱 평균 오차\n",
    "- MAE: Mean Absolute Error 절대값 평균 오차\n",
    "\n",
    "러프하게 설명하자면, 하나의 attribute에 의해 선형 회귀되는 target에 대해서, 이에 대한 가중치 a, b를 변경시켰을 때, mse혹은 mae의 값이 최소가 되는 가중치를 산출하는 방법이다.\n",
    "    - 손실함수를 최소화하기 위하여 반복적으로 파라미터를 조정해나간다.\n",
    "    - 새로운 파라메터 = Y – delta* (Y미분값) //delta는 학습률 ,즉 자기의 기울기 값만큼 이동하여 최솟값을 찾는다.\n",
    "\n",
    "> [!note] 그래디언트\n",
    "> \n",
    "> 그래디언트는 다변수 함수의 모든 편도함수를 벡터로 표현한 것. 함수 f(x, y)의 그래디언트는 다음과 같다.\n",
    "> ∇f = [∂f/∂x, ∂f/∂y]\n",
    "> 여기서 ∂f/∂x는 x에 대한 편도함수, ∂f/∂y는 y에 대한 편도함수이다.\n",
    "\n",
    "**그래디언트**와 **손실 함수**에 대해 수학 개념을 찾아보면 아래 수식이 나온 이유를 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69d6053b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, 기울기 = 23.1998, 절편 = 4.5253, 에러 = 90.4986\n",
      "epoch = 10, 기울기 = 14.0342, 절편 = 9.0852, 에러 = 24.6581\n",
      "epoch = 20, 기울기 = 13.1004, 절편 = 14.5481, 에러 = 22.8771\n",
      "epoch = 30, 기울기 = 12.2560, 절편 = 19.5866, 에러 = 21.1748\n",
      "epoch = 40, 기울기 = 11.4777, 절편 = 24.2312, 에러 = 19.6055\n",
      "epoch = 50, 기울기 = 10.7603, 절편 = 28.5127, 에러 = 18.1588\n",
      "epoch = 60, 기울기 = 10.0989, 절편 = 32.4596, 에러 = 16.8253\n",
      "epoch = 70, 기울기 = 9.4892, 절편 = 36.0978, 에러 = 15.5959\n",
      "epoch = 80, 기울기 = 8.9272, 절편 = 39.4517, 에러 = 14.4627\n",
      "epoch = 90, 기울기 = 8.4091, 절편 = 42.5434, 에러 = 13.4181\n",
      "epoch = 100, 기울기 = 7.9315, 절편 = 45.3934, 에러 = 12.4551\n",
      "epoch = 110, 기울기 = 7.4913, 절편 = 48.0206, 에러 = 11.5675\n",
      "epoch = 120, 기울기 = 7.0855, 절편 = 50.4424, 에러 = 10.7492\n",
      "epoch = 130, 기울기 = 6.7114, 절편 = 52.6749, 에러 = 9.9948\n",
      "epoch = 140, 기울기 = 6.3665, 절편 = 54.7328, 에러 = 9.2995\n",
      "epoch = 150, 기울기 = 6.0486, 절편 = 56.6299, 에러 = 8.6585\n",
      "epoch = 160, 기울기 = 5.7556, 절편 = 58.3787, 에러 = 8.0676\n",
      "epoch = 170, 기울기 = 5.4854, 절편 = 59.9907, 에러 = 7.5229\n",
      "epoch = 180, 기울기 = 5.2364, 절편 = 61.4768, 에러 = 7.0208\n",
      "epoch = 190, 기울기 = 5.0069, 절편 = 62.8467, 에러 = 6.5580\n",
      "epoch = 200, 기울기 = 4.7952, 절편 = 64.1094, 에러 = 6.1313\n",
      "epoch = 210, 기울기 = 4.6002, 절편 = 65.2735, 에러 = 5.7380\n",
      "epoch = 220, 기울기 = 4.4204, 절편 = 66.3466, 에러 = 5.3754\n",
      "epoch = 230, 기울기 = 4.2546, 절편 = 67.3357, 에러 = 5.0412\n",
      "epoch = 240, 기울기 = 4.1018, 절편 = 68.2476, 에러 = 4.7331\n",
      "epoch = 250, 기울기 = 3.9609, 절편 = 69.0882, 에러 = 4.4491\n",
      "epoch = 260, 기울기 = 3.8311, 절편 = 69.8630, 에러 = 4.1872\n",
      "epoch = 270, 기울기 = 3.7114, 절편 = 70.5773, 에러 = 3.9459\n",
      "epoch = 280, 기울기 = 3.6011, 절편 = 71.2357, 에러 = 3.7234\n",
      "epoch = 290, 기울기 = 3.4994, 절편 = 71.8427, 에러 = 3.5183\n",
      "epoch = 300, 기울기 = 3.4056, 절편 = 72.4022, 에러 = 3.3293\n",
      "epoch = 310, 기울기 = 3.3192, 절편 = 72.9180, 에러 = 3.1550\n",
      "epoch = 320, 기울기 = 3.2395, 절편 = 73.3935, 에러 = 2.9944\n",
      "epoch = 330, 기울기 = 3.1661, 절편 = 73.8318, 에러 = 2.8463\n",
      "epoch = 340, 기울기 = 3.0984, 절편 = 74.2358, 에러 = 2.7098\n",
      "epoch = 350, 기울기 = 3.0359, 절편 = 74.6082, 에러 = 2.5839\n",
      "epoch = 360, 기울기 = 2.9784, 절편 = 74.9515, 에러 = 2.4679\n",
      "epoch = 370, 기울기 = 2.9254, 절편 = 75.2680, 에러 = 2.4102\n",
      "epoch = 380, 기울기 = 2.8765, 절편 = 75.5598, 에러 = 2.4094\n",
      "epoch = 390, 기울기 = 2.8314, 절편 = 75.8287, 에러 = 2.4087\n",
      "epoch = 400, 기울기 = 2.7899, 절편 = 76.0766, 에러 = 2.4080\n",
      "epoch = 410, 기울기 = 2.7516, 절편 = 76.3052, 에러 = 2.4074\n",
      "epoch = 420, 기울기 = 2.7163, 절편 = 76.5158, 에러 = 2.4068\n",
      "epoch = 430, 기울기 = 2.6837, 절편 = 76.7100, 에러 = 2.4063\n",
      "epoch = 440, 기울기 = 2.6537, 절편 = 76.8891, 에러 = 2.4058\n",
      "epoch = 450, 기울기 = 2.6261, 절편 = 77.0541, 에러 = 2.4053\n",
      "epoch = 460, 기울기 = 2.6006, 절편 = 77.2062, 에러 = 2.4049\n",
      "epoch = 470, 기울기 = 2.5771, 절편 = 77.3464, 에러 = 2.4045\n",
      "epoch = 480, 기울기 = 2.5554, 절편 = 77.4757, 에러 = 2.4042\n",
      "epoch = 490, 기울기 = 2.5355, 절편 = 77.5949, 에러 = 2.4038\n",
      "epoch = 500, 기울기 = 2.5171, 절편 = 77.7047, 에러 = 2.4035\n",
      "epoch = 510, 기울기 = 2.5001, 절편 = 77.8060, 에러 = 2.4033\n",
      "epoch = 520, 기울기 = 2.4844, 절편 = 77.8993, 에러 = 2.4030\n",
      "epoch = 530, 기울기 = 2.4700, 절편 = 77.9854, 에러 = 2.4028\n",
      "epoch = 540, 기울기 = 2.4567, 절편 = 78.0647, 에러 = 2.4026\n",
      "epoch = 550, 기울기 = 2.4445, 절편 = 78.1378, 에러 = 2.4024\n",
      "epoch = 560, 기울기 = 2.4332, 절편 = 78.2052, 에러 = 2.4022\n",
      "epoch = 570, 기울기 = 2.4228, 절편 = 78.2673, 에러 = 2.4020\n",
      "epoch = 580, 기울기 = 2.4132, 절편 = 78.3246, 에러 = 2.4019\n",
      "epoch = 590, 기울기 = 2.4043, 절편 = 78.3774, 에러 = 2.4017\n",
      "epoch = 600, 기울기 = 2.3962, 절편 = 78.4261, 에러 = 2.4016\n",
      "epoch = 610, 기울기 = 2.3887, 절편 = 78.4709, 에러 = 2.4014\n",
      "epoch = 620, 기울기 = 2.3817, 절편 = 78.5123, 에러 = 2.4013\n",
      "epoch = 630, 기울기 = 2.3753, 절편 = 78.5504, 에러 = 2.4012\n",
      "epoch = 640, 기울기 = 2.3694, 절편 = 78.5856, 에러 = 2.4011\n",
      "epoch = 650, 기울기 = 2.3640, 절편 = 78.6180, 에러 = 2.4010\n",
      "epoch = 660, 기울기 = 2.3590, 절편 = 78.6478, 에러 = 2.4010\n",
      "epoch = 670, 기울기 = 2.3544, 절편 = 78.6754, 에러 = 2.4009\n",
      "epoch = 680, 기울기 = 2.3501, 절편 = 78.7007, 에러 = 2.4008\n",
      "epoch = 690, 기울기 = 2.3462, 절편 = 78.7241, 에러 = 2.4008\n",
      "epoch = 700, 기울기 = 2.3426, 절편 = 78.7457, 에러 = 2.4007\n",
      "epoch = 710, 기울기 = 2.3393, 절편 = 78.7656, 에러 = 2.4006\n",
      "epoch = 720, 기울기 = 2.3362, 절편 = 78.7839, 에러 = 2.4006\n",
      "epoch = 730, 기울기 = 2.3334, 절편 = 78.8008, 에러 = 2.4005\n",
      "epoch = 740, 기울기 = 2.3308, 절편 = 78.8164, 에러 = 2.4005\n",
      "epoch = 750, 기울기 = 2.3284, 절편 = 78.8307, 에러 = 2.4005\n",
      "epoch = 760, 기울기 = 2.3261, 절편 = 78.8440, 에러 = 2.4004\n",
      "epoch = 770, 기울기 = 2.3241, 절편 = 78.8562, 에러 = 2.4004\n",
      "epoch = 780, 기울기 = 2.3222, 절편 = 78.8674, 에러 = 2.4004\n",
      "epoch = 790, 기울기 = 2.3205, 절편 = 78.8778, 에러 = 2.4003\n",
      "epoch = 800, 기울기 = 2.3189, 절편 = 78.8873, 에러 = 2.4003\n",
      "epoch = 810, 기울기 = 2.3174, 절편 = 78.8961, 에러 = 2.4003\n",
      "epoch = 820, 기울기 = 2.3160, 절편 = 78.9043, 에러 = 2.4003\n",
      "epoch = 830, 기울기 = 2.3148, 절편 = 78.9117, 에러 = 2.4002\n",
      "epoch = 840, 기울기 = 2.3136, 절편 = 78.9186, 에러 = 2.4002\n",
      "epoch = 850, 기울기 = 2.3126, 절편 = 78.9250, 에러 = 2.4002\n",
      "epoch = 860, 기울기 = 2.3116, 절편 = 78.9309, 에러 = 2.4002\n",
      "epoch = 870, 기울기 = 2.3107, 절편 = 78.9363, 에러 = 2.4002\n",
      "epoch = 880, 기울기 = 2.3098, 절편 = 78.9412, 에러 = 2.4002\n",
      "epoch = 890, 기울기 = 2.3091, 절편 = 78.9458, 에러 = 2.4001\n",
      "epoch = 900, 기울기 = 2.3084, 절편 = 78.9501, 에러 = 2.4001\n",
      "epoch = 910, 기울기 = 2.3077, 절편 = 78.9540, 에러 = 2.4001\n",
      "epoch = 920, 기울기 = 2.3071, 절편 = 78.9576, 에러 = 2.4001\n",
      "epoch = 930, 기울기 = 2.3066, 절편 = 78.9609, 에러 = 2.4001\n",
      "epoch = 940, 기울기 = 2.3060, 절편 = 78.9639, 에러 = 2.4001\n",
      "epoch = 950, 기울기 = 2.3056, 절편 = 78.9668, 에러 = 2.4001\n",
      "epoch = 960, 기울기 = 2.3051, 절편 = 78.9694, 에러 = 2.4001\n",
      "epoch = 970, 기울기 = 2.3047, 절편 = 78.9718, 에러 = 2.4001\n",
      "epoch = 980, 기울기 = 2.3044, 절편 = 78.9740, 에러 = 2.4001\n",
      "epoch = 990, 기울기 = 2.3040, 절편 = 78.9760, 에러 = 2.4001\n",
      "최종기울기 = 2.3037 절편 = 78.9777, 에러 = 2.4001\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([2, 4, 6, 8])\n",
    "y_data = np.array([81, 93, 91, 97])\n",
    "\n",
    "a = 0.0002\n",
    "b = 0.0004\n",
    "\n",
    "learning_rate = 0.05\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred = a * x_data + b\n",
    "    error = np.abs(y_pred - y_data).mean() # mae\n",
    "    # error = np.power(y_pred-y_data, 2).mean() # mse\n",
    "    if error < 0.001: # 학습 종료 조건\n",
    "        break\n",
    "    a_grad = learning_rate * ((y_pred - y_data)*x_data).mean() # 그래디언트 산출: \n",
    "    b_grad = learning_rate * (y_pred - y_data).mean()\n",
    "\n",
    "    a = a - a_grad # 기울기 재반영\n",
    "    b = b - b_grad # 절편 재반영\n",
    "    \n",
    "    if epoch % 10 == 0: # 10의 배수인 epoch 마다 출력 \n",
    "        print(\"epoch = %.f, 기울기 = %.04f, 절편 = %.04f, 에러 = %.04f\" % (epoch, a, b, error.mean()))\n",
    "\n",
    "print(\"최종기울기 = %.04f 절편 = %.04f, 에러 = %.04f\" % (a, b, error.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "996ffb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_data, y_data)\n",
    "\n",
    "start_x = x_data[0]\n",
    "start_y = a*start_x+b\n",
    "\n",
    "end_x = x_data[-1]\n",
    "end_y = a*end_x+b\n",
    "\n",
    "# Linear Regression으로 만든 기울기와 절편 값으로 첫 x 데이터 / 마지막 x 데이터로 두개의 점을 만들어 연결하였다.\n",
    "plt.plot([start_x, end_x], [start_y, end_y]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff24dd",
   "metadata": {},
   "source": [
    "### 경사하강: Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fca4b823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기울기 a: [[-0.06893349 -0.07721176  0.18263033  0.66273159]] y절편 b: [0.16585107]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris['data'], columns=iris['feature_names'])\n",
    "df['target'] = iris['target']\n",
    "\n",
    "df_shuffled = df.iloc[np.random.permutation(df.index)].reset_index(drop=True)\n",
    "\n",
    "x_train = df_shuffled.iloc[:120, :-1]\n",
    "x_test  = df_shuffled.iloc[120:, :-1]\n",
    "y_train = df_shuffled.iloc[:120, -1:]\n",
    "y_test  = df_shuffled.iloc[120:, -1:]\n",
    "\n",
    "LR = LinearRegression()\n",
    "LR.fit(x_train, y_train)\n",
    "\n",
    "print(\"기울기 a:\", LR.coef_, \"y절편 b:\", LR.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7a9026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK : 1\t1\n",
      "OK : 0\t0\n",
      "OK : 1\t1\n",
      "OK : 0\t0\n",
      "OK : 0\t0\n",
      "OK : 1\t1\n",
      "OK : 0\t0\n",
      "OK : 0\t0\n",
      "OK : 2\t2\n",
      "OK : 0\t0\n",
      "OK : 0\t0\n",
      "OK : 2\t2\n",
      "OK : 1\t1\n",
      "OK : 0\t0\n",
      "OK : 1\t1\n",
      "OK : 0\t0\n",
      "OK : 1\t1\n",
      "OK : 1\t1\n",
      "===> NOK: 2\t1\n",
      "OK : 1\t1\n",
      "OK : 0\t0\n",
      "OK : 1\t1\n",
      "OK : 0\t0\n",
      "OK : 2\t2\n",
      "OK : 1\t1\n",
      "OK : 2\t2\n",
      "OK : 1\t1\n",
      "OK : 0\t0\n",
      "OK : 2\t2\n",
      "OK : 2\t2\n",
      "총 30개 중  29 개 예측 성공 정확도(96.67%)\n"
     ]
    }
   ],
   "source": [
    "def y_pred_f(x1, x2, x3, x4):\n",
    "    return(int(np.round(LR.coef_[-1,0]*x1 + LR.coef_[-1,1]*x2 +\n",
    "               LR.coef_[-1,2]*x3 + LR.coef_[-1,3]*x4 + \n",
    "               LR.intercept_[-1])))\n",
    "\n",
    "ok_cnt = 0\n",
    "for i in range(len(x_test)):\n",
    "    y_pred = y_pred_f(x_test.iloc[i,0], x_test.iloc[i,1],\n",
    "                      x_test.iloc[i,2], x_test.iloc[i,3])\n",
    "    if int(y_test.iloc[i,-1]) == y_pred:\n",
    "        ok_cnt = ok_cnt + 1\n",
    "        print(f\"OK : %d\\t%d\"%(y_test.iloc[i,-1], y_pred))\n",
    "    else:\n",
    "        print(f\"===> NOK: %d\\t%d\"%(y_test.iloc[i,-1],y_pred))\n",
    "\n",
    "print(\"총 30개 중 \", ok_cnt, \"개 예측 성공 정확도(%.2f%%)\"%((ok_cnt/30)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5a3872",
   "metadata": {},
   "source": [
    "### 경사하강: Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "055378c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기울기 a: [[ -15.90469239 -238.12134362  439.71595596  412.6808245  -678.52985894\n",
      "   421.59082468   15.70301756  100.51318282  711.43758102  106.91525897]] y절편 b: [148.81254646]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "df = pd.DataFrame(diabetes['data'], columns=diabetes['feature_names'])\n",
    "df['target'] = diabetes['target']\n",
    "\n",
    "df_shuffled = df.iloc[np.random.permutation(df.index)].reset_index(drop=True)\n",
    "\n",
    "x_train = df_shuffled.iloc[:309, :-1]\n",
    "x_test  = df_shuffled.iloc[309:, :-1]\n",
    "y_train = df_shuffled.iloc[:309, -1:]\n",
    "y_test  = df_shuffled.iloc[309:, -1:]\n",
    "\n",
    "LR = LinearRegression()\n",
    "LR.fit(x_train, y_train)\n",
    "\n",
    "print(\"기울기 a:\", LR.coef_, \"y절편 b:\", LR.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00c85df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 값과의 오차율: 36.89%\n",
      "실제 값과의 오차율: 28.04%\n",
      "실제 값과의 오차율: 20.65%\n",
      "실제 값과의 오차율: 13.20%\n",
      "실제 값과의 오차율: 34.19%\n",
      "실제 값과의 오차율: 31.18%\n",
      "실제 값과의 오차율: 2.59%\n",
      "실제 값과의 오차율: 24.73%\n",
      "실제 값과의 오차율: 2.94%\n",
      "실제 값과의 오차율: 4.60%\n",
      "실제 값과의 오차율: 20.09%\n",
      "실제 값과의 오차율: 49.55%\n",
      "실제 값과의 오차율: 2.76%\n",
      "실제 값과의 오차율: 190.00%\n",
      "실제 값과의 오차율: 46.38%\n",
      "실제 값과의 오차율: 29.55%\n",
      "실제 값과의 오차율: 28.64%\n",
      "실제 값과의 오차율: 21.03%\n",
      "실제 값과의 오차율: 41.54%\n",
      "실제 값과의 오차율: 177.61%\n",
      "실제 값과의 오차율: 137.74%\n",
      "실제 값과의 오차율: 42.00%\n",
      "실제 값과의 오차율: 18.28%\n",
      "실제 값과의 오차율: 40.24%\n",
      "실제 값과의 오차율: 9.13%\n",
      "실제 값과의 오차율: 11.69%\n",
      "실제 값과의 오차율: 0.00%\n",
      "실제 값과의 오차율: 35.74%\n",
      "실제 값과의 오차율: 10.47%\n",
      "실제 값과의 오차율: 2.14%\n",
      "실제 값과의 오차율: 20.48%\n",
      "실제 값과의 오차율: 46.38%\n",
      "실제 값과의 오차율: 10.00%\n",
      "실제 값과의 오차율: 12.21%\n",
      "실제 값과의 오차율: 2.42%\n",
      "실제 값과의 오차율: 5.15%\n",
      "실제 값과의 오차율: 64.71%\n",
      "실제 값과의 오차율: 28.57%\n",
      "실제 값과의 오차율: 17.79%\n",
      "실제 값과의 오차율: 19.05%\n",
      "실제 값과의 오차율: 16.05%\n",
      "실제 값과의 오차율: 57.69%\n",
      "실제 값과의 오차율: 21.64%\n",
      "실제 값과의 오차율: 23.53%\n",
      "실제 값과의 오차율: 48.03%\n",
      "실제 값과의 오차율: 35.71%\n",
      "실제 값과의 오차율: 101.54%\n",
      "실제 값과의 오차율: 9.16%\n",
      "실제 값과의 오차율: 12.00%\n",
      "실제 값과의 오차율: 31.68%\n",
      "실제 값과의 오차율: 33.33%\n",
      "실제 값과의 오차율: 181.03%\n",
      "실제 값과의 오차율: 51.15%\n",
      "실제 값과의 오차율: 3.54%\n",
      "실제 값과의 오차율: 16.07%\n",
      "실제 값과의 오차율: 16.67%\n",
      "실제 값과의 오차율: 15.88%\n",
      "실제 값과의 오차율: 8.26%\n",
      "실제 값과의 오차율: 16.20%\n",
      "실제 값과의 오차율: 32.50%\n",
      "실제 값과의 오차율: 79.01%\n",
      "실제 값과의 오차율: 38.49%\n",
      "실제 값과의 오차율: 44.39%\n",
      "실제 값과의 오차율: 12.50%\n",
      "실제 값과의 오차율: 49.80%\n",
      "실제 값과의 오차율: 13.51%\n",
      "실제 값과의 오차율: 22.73%\n",
      "실제 값과의 오차율: 37.21%\n",
      "실제 값과의 오차율: 3.96%\n",
      "실제 값과의 오차율: 26.46%\n",
      "실제 값과의 오차율: 60.20%\n",
      "실제 값과의 오차율: 2.97%\n",
      "실제 값과의 오차율: 11.76%\n",
      "실제 값과의 오차율: 38.80%\n",
      "실제 값과의 오차율: 64.00%\n",
      "실제 값과의 오차율: 280.77%\n",
      "실제 값과의 오차율: 21.82%\n",
      "실제 값과의 오차율: 27.87%\n",
      "실제 값과의 오차율: 17.33%\n",
      "실제 값과의 오차율: 49.09%\n",
      "실제 값과의 오차율: 14.91%\n",
      "실제 값과의 오차율: 11.32%\n",
      "실제 값과의 오차율: 41.67%\n",
      "실제 값과의 오차율: 6.90%\n",
      "실제 값과의 오차율: 121.67%\n",
      "실제 값과의 오차율: 13.57%\n",
      "실제 값과의 오차율: 16.23%\n",
      "실제 값과의 오차율: 4.68%\n",
      "실제 값과의 오차율: 37.60%\n",
      "실제 값과의 오차율: 7.27%\n",
      "실제 값과의 오차율: 41.41%\n",
      "실제 값과의 오차율: 44.58%\n",
      "실제 값과의 오차율: 70.31%\n",
      "실제 값과의 오차율: 46.09%\n",
      "실제 값과의 오차율: 0.00%\n",
      "실제 값과의 오차율: 28.61%\n",
      "실제 값과의 오차율: 96.83%\n",
      "실제 값과의 오차율: 26.52%\n",
      "실제 값과의 오차율: 32.20%\n",
      "실제 값과의 오차율: 70.21%\n",
      "실제 값과의 오차율: 234.62%\n",
      "실제 값과의 오차율: 37.43%\n",
      "실제 값과의 오차율: 35.24%\n",
      "실제 값과의 오차율: 19.61%\n",
      "실제 값과의 오차율: 29.92%\n",
      "실제 값과의 오차율: 50.82%\n",
      "실제 값과의 오차율: 1.30%\n",
      "실제 값과의 오차율: 1.94%\n",
      "실제 값과의 오차율: 18.92%\n",
      "실제 값과의 오차율: 32.08%\n",
      "실제 값과의 오차율: 28.07%\n",
      "실제 값과의 오차율: 6.82%\n",
      "실제 값과의 오차율: 25.90%\n",
      "실제 값과의 오차율: 25.93%\n",
      "실제 값과의 오차율: 18.33%\n",
      "실제 값과의 오차율: 44.25%\n",
      "실제 값과의 오차율: 82.05%\n",
      "실제 값과의 오차율: 15.27%\n",
      "실제 값과의 오차율: 4.42%\n",
      "실제 값과의 오차율: 35.04%\n",
      "실제 값과의 오차율: 5.76%\n",
      "실제 값과의 오차율: 6.29%\n",
      "실제 값과의 오차율: 12.10%\n",
      "실제 값과의 오차율: 32.69%\n",
      "실제 값과의 오차율: 6.34%\n",
      "실제 값과의 오차율: 27.78%\n",
      "실제 값과의 오차율: 52.94%\n",
      "실제 값과의 오차율: 13.79%\n",
      "실제 값과의 오차율: 32.71%\n",
      "실제 값과의 오차율: 1.92%\n",
      "실제 값과의 오차율: 21.05%\n",
      "실제 값과의 오차율: 4.67%\n",
      "실제 값과의 오차율: 2.87%\n",
      "총 133개의 실제 값과 예측 값의 오차율 평균: (35.16%)\n"
     ]
    }
   ],
   "source": [
    "def y_pred_f(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10):\n",
    "    return(int(np.round(LR.coef_[-1,0]*x1 + LR.coef_[-1,1]*x2 +\n",
    "               LR.coef_[-1,2]*x3 + LR.coef_[-1,3]*x4 + LR.coef_[-1,4]*x5 + LR.coef_[-1,5]*x6 +\n",
    "               LR.coef_[-1,6]*x7 + LR.coef_[-1,7]*x8 + LR.coef_[-1,8]*x9 + LR.coef_[-1,9]*x10 +\n",
    "               LR.intercept_[-1])))\n",
    "\n",
    "diff_sum = 0\n",
    "for i in range(len(x_test)):\n",
    "    y_pred = y_pred_f(x_test.iloc[i,0], x_test.iloc[i,1],\n",
    "                      x_test.iloc[i,2], x_test.iloc[i,3], x_test.iloc[i,4], x_test.iloc[i,5],\n",
    "                      x_test.iloc[i,6], x_test.iloc[i,7],x_test.iloc[i,8], x_test.iloc[i,9])\n",
    "    diff = (np.abs(y_test.iloc[i, -1] - y_pred)/y_test.iloc[i,-1]*100)\n",
    "    diff_sum = diff_sum + diff\n",
    "    print(\"실제 값과의 오차율: %.2f%%\"%(diff))\n",
    "\n",
    "print(\"총 133개의 실제 값과 예측 값의 오차율 평균: (%.2f%%)\"%(diff_sum / 133))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3664393a",
   "metadata": {},
   "source": [
    "### 경사하강: Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "811a3d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=9000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=9000)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=9000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "# 세트는 총 569개, 399개는 train, 나머지 170개는 test로 설정하려고 한다.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "# 데이터프레임으로 정의\n",
    "df = pd.DataFrame(breast_cancer['data'], columns=breast_cancer['feature_names'])\n",
    "df['target']=breast_cancer['target']\n",
    "\n",
    "df_shuffled = df.iloc[np.random.permutation(df.index)].reset_index(drop=True)\n",
    "\n",
    "x_train = df_shuffled.iloc[:399, :-1]\n",
    "x_test  = df_shuffled.iloc[399:, :-1]\n",
    "y_train = df_shuffled.iloc[:399, -1]\n",
    "y_test  = df_shuffled.iloc[399:, -1]\n",
    "\n",
    "LR = LogisticRegression(max_iter=9000)\n",
    "LR.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0cbadaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 30개 중  22 개 예측 성공(73.33%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = LR.predict(x_test)\n",
    "\n",
    "comparison = y_test == y_pred\n",
    "differences = (~comparison).sum()\n",
    "\n",
    "print(\"총 30개 중 \", 30-differences, \"개 예측 성공(%.2f%%)\"%((1-differences/30)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
